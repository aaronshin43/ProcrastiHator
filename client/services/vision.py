# client/services/vision.py
import sys
import os
from PyQt6.QtCore import QThread, pyqtSignal
import cv2
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
from mediapipe import Image as MPImage
from mediapipe.tasks.python.vision.core.image import ImageFormat
import numpy as np
import time

# shared í´ë” importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from shared.protocol import Packet, PacketMeta
from shared.constants import VisionEvents, PacketCategory

class VisionWorker(QThread):
    # ë©”ì¸ UIë¡œ ë³´ë‚¼ ì‹ í˜¸ ì •ì˜
    alert_signal = pyqtSignal(object) # Packet ê°ì²´ë¥¼ ë³´ëƒ„

    def __init__(self, show_debug_window=False):
        super().__init__()
        self.running = False
        self.show_debug_window = show_debug_window  # ë””ë²„ê·¸ ì°½ í‘œì‹œ ì—¬ë¶€
        # MediaPipe Face Landmarker ì´ˆê¸°í™” (0.10.x API)
        # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        model_path = os.path.join(os.path.dirname(__file__), 'face_landmarker.task')
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(
                f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}\n"
                f"ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”: python download_mediapipe_model.py"
            )
        
        base_options = python.BaseOptions(model_asset_path=model_path)
        
        options = vision.FaceLandmarkerOptions(
            base_options=base_options,
            output_face_blendshapes=False,
            output_facial_transformation_matrixes=False,
            num_faces=1,
            min_face_detection_confidence=0.5,
            min_face_presence_confidence=0.5,
            min_tracking_confidence=0.5,
            running_mode=vision.RunningMode.IMAGE  # ì´ë¯¸ì§€ ëª¨ë“œ
        )
        
        try:
            self.face_landmarker = vision.FaceLandmarker.create_from_options(options)
        except Exception as e:
            print(f"âš ï¸ FaceLandmarker ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ëª¨ë¸ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê±°ë‚˜ ë‹¤ë¥¸ ë°©ë²•ì„ ì‹œë„í•´ì£¼ì„¸ìš”.")
            raise
        
        # ìƒíƒœ ì¶”ì  (ì¹´ìš´í„° ë°©ì‹)
        self.eye_closed_counter = 0  # ëˆˆ ê°ìŒ ì—°ì† í”„ë ˆì„ ì¹´ìš´í„°
        self.no_face_counter = 0  # ì–¼êµ´ ë¶€ì¬ ì—°ì† í”„ë ˆì„ ì¹´ìš´í„°
        self.last_alert_time = {}  # ê° ì´ë²¤íŠ¸ë³„ ë§ˆì§€ë§‰ ì•Œë¦¼ ì‹œê°„ (ì¤‘ë³µ ë°©ì§€)
        
        # EAR ì„ê³„ê°’
        self.EAR_THRESHOLD = 0.25  # ëˆˆ ê°ìŒ ì„ê³„ê°’
        self.EAR_CONSECUTIVE_FRAMES = 100  # ì—°ì† í”„ë ˆì„ ìˆ˜ (ì¡¸ìŒ ê°ì§€ ì„ê³„ê°’)
        self.NO_FACE_CONSECUTIVE_FRAMES = 100  # ì–¼êµ´ ë¶€ì¬ ì—°ì† í”„ë ˆì„ ìˆ˜
        
        # ëˆˆ ëœë“œë§ˆí¬ ì¸ë±ìŠ¤ (EAR ê³„ì‚°ìš©)
        self.LEFT_EYE_EAR = [33, 160, 158, 133, 153, 144]
        self.RIGHT_EYE_EAR = [362, 385, 387, 263, 390, 374]
        
        # ì–¼êµ´ ë°©í–¥ ê³„ì‚°ìš© ëœë“œë§ˆí¬
        self.NOSE_TIP = 1
        self.CHIN = 175
        self.LEFT_EYE_CENTER = 33
        self.RIGHT_EYE_CENTER = 362
    
    def calculate_ear(self, landmarks, eye_indices):
        """Eye Aspect Ratio (EAR) ê³„ì‚°"""
        # MediaPipe 0.10.xëŠ” landmarksê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ
        # ìˆ˜ì§ ê±°ë¦¬ ê³„ì‚°
        vertical_1 = np.linalg.norm(
            np.array([landmarks[eye_indices[1]].x, landmarks[eye_indices[1]].y]) -
            np.array([landmarks[eye_indices[5]].x, landmarks[eye_indices[5]].y])
        )
        vertical_2 = np.linalg.norm(
            np.array([landmarks[eye_indices[2]].x, landmarks[eye_indices[2]].y]) -
            np.array([landmarks[eye_indices[4]].x, landmarks[eye_indices[4]].y])
        )
        
        # ìˆ˜í‰ ê±°ë¦¬ ê³„ì‚°
        horizontal = np.linalg.norm(
            np.array([landmarks[eye_indices[0]].x, landmarks[eye_indices[0]].y]) -
            np.array([landmarks[eye_indices[3]].x, landmarks[eye_indices[3]].y])
        )
        
        # EAR ê³„ì‚°
        if horizontal == 0:
            return 0.0
        ear = (vertical_1 + vertical_2) / (2.0 * horizontal)
        return ear
    
    def should_alert(self, event_type, cooldown_seconds=5):
        """ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€ (ì¿¨ë‹¤ìš´)"""
        current_time = time.time()
        last_time = self.last_alert_time.get(event_type, 0)
        
        if current_time - last_time < cooldown_seconds:
            return False
        
        self.last_alert_time[event_type] = current_time
        return True
    
    def calculate_face_orientation(self, landmarks, frame_width, frame_height):
        """ì–¼êµ´ ë°©í–¥ ê³„ì‚° (pitch, yaw)"""
        # 3D ëœë“œë§ˆí¬ì—ì„œ ì–¼êµ´ ë°©í–¥ ì¶”ì •
        nose_tip = landmarks[self.NOSE_TIP]
        chin = landmarks[self.CHIN]
        left_eye = landmarks[self.LEFT_EYE_CENTER]
        right_eye = landmarks[self.RIGHT_EYE_CENTER]
        
        # ëˆˆ ì¤‘ì‹¬ì 
        eye_center_x = (left_eye.x + right_eye.x) / 2
        eye_center_y = (left_eye.y + right_eye.y) / 2
        
        # ì–¼êµ´ ì¤‘ì‹¬ì 
        face_center_x = (eye_center_x + chin.x) / 2
        face_center_y = (eye_center_y + chin.y) / 2
        
        # í”„ë ˆì„ ì¤‘ì‹¬ê³¼ì˜ ì°¨ì´ ê³„ì‚°
        frame_center_x = 0.5
        frame_center_y = 0.5
        
        # Yaw (ì¢Œìš° íšŒì „)
        yaw = (face_center_x - frame_center_x) * 2  # -1 ~ 1 ë²”ìœ„
        
        # Pitch (ìƒí•˜ íšŒì „)
        pitch = (face_center_y - frame_center_y) * 2  # -1 ~ 1 ë²”ìœ„
        
        return pitch, yaw
    
    def draw_debug_info(self, frame, face_landmarks, avg_ear, pitch, yaw, is_sleeping, is_absent):
        """ë””ë²„ê·¸ ì •ë³´ë¥¼ í”„ë ˆì„ì— ê·¸ë¦¬ê¸°"""
        frame_height, frame_width = frame.shape[:2]
        
        # ìƒíƒœ ì •ë³´ í…ìŠ¤íŠ¸
        y_offset = 30
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        thickness = 2
        
        # ì–¼êµ´ ê°ì§€ ì—¬ë¶€ í‘œì‹œ
        if face_landmarks:
            # ì–¼êµ´ì´ ê°ì§€ë¨
            cv2.putText(frame, "Face: DETECTED", (10, y_offset),
                       font, font_scale, (0, 255, 0), thickness)
            
            # ëˆˆ ì˜ì—­ ê·¸ë¦¬ê¸°
            for eye_indices in [self.LEFT_EYE_EAR, self.RIGHT_EYE_EAR]:
                eye_points = []
                for idx in eye_indices:
                    if idx < len(face_landmarks):
                        point = face_landmarks[idx]
                        x = int(point.x * frame_width)
                        y = int(point.y * frame_height)
                        eye_points.append((x, y))
                        cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)
                
                # ëˆˆ ìœ¤ê³½ì„  ê·¸ë¦¬ê¸°
                if len(eye_points) >= 4:
                    pts = np.array(eye_points, np.int32)
                    cv2.polylines(frame, [pts], True, (0, 255, 0), 1)
            
            y_offset += 25
            
            # EAR ê°’
            ear_color = (0, 255, 0) if avg_ear >= self.EAR_THRESHOLD else (0, 0, 255)
            cv2.putText(frame, f"EAR: {avg_ear:.3f}", (10, y_offset), 
                       font, font_scale, ear_color, thickness)
            y_offset += 25
            
            # ì–¼êµ´ ë°©í–¥
            cv2.putText(frame, f"Pitch: {pitch:.2f}, Yaw: {yaw:.2f}", (10, y_offset),
                       font, font_scale, (255, 255, 255), thickness)
            y_offset += 25
            
            # ëˆˆ ê°ìŒ í”„ë ˆì„ ìˆ˜
            cv2.putText(frame, f"Closed Frames: {self.eye_closed_counter}/{self.EAR_CONSECUTIVE_FRAMES}", 
                       (10, y_offset), font, font_scale, (255, 255, 255), thickness)
            y_offset += 25
        else:
            # ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•ŠìŒ
            cv2.putText(frame, "Face: NOT DETECTED", (10, y_offset),
                       font, font_scale, (0, 0, 255), thickness)
            y_offset += 25
            
            # ì–¼êµ´ ë¶€ì¬ í”„ë ˆì„ ìˆ˜
            cv2.putText(frame, f"No Face Frames: {self.no_face_counter}/{self.NO_FACE_CONSECUTIVE_FRAMES}", 
                       (10, y_offset), font, font_scale, (255, 255, 255), thickness)
            y_offset += 25
        
        # ìƒíƒœ í‘œì‹œ (í•­ìƒ í‘œì‹œ)
        if is_sleeping:
            cv2.putText(frame, "SLEEPING!", (10, y_offset),
                       font, 0.8, (0, 0, 255), 2)
            y_offset += 30
        if is_absent:
            cv2.putText(frame, "ABSENT!", (10, y_offset),
                       font, 0.8, (255, 0, 0), 2)
            y_offset += 30
        
        return frame

    def run(self):
        self.running = True
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("[ERROR] ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›¹ìº ì´ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            self.running = False
            return
        
        print("[OK] ì›¹ìº  ì—°ê²° ì„±ê³µ - Vision Worker ì‹œì‘")
        
        try:
            while self.running:
                try:
                    ret, frame = cap.read()
                    if not ret:
                        print("[WARNING] í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        continue
                    
                    # MediaPipe Face Landmarker ì²˜ë¦¬
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    mp_image = MPImage(image_format=ImageFormat.SRGB, data=frame_rgb)
                    detection_result = self.face_landmarker.detect(mp_image)
                    
                    is_sleeping = False
                    is_absent = False
                    avg_ear = 0.0  # ê¸°ë³¸ê°’
                    pitch, yaw = 0.0, 0.0  # ì–¼êµ´ ë°©í–¥
                    
                    if detection_result.face_landmarks:
                        # ì–¼êµ´ì´ ê°ì§€ë¨ - ì–¼êµ´ ë¶€ì¬ ì¹´ìš´í„° ë¦¬ì…‹
                        self.no_face_counter = 0
                        face_landmarks = detection_result.face_landmarks[0]  # ì²« ë²ˆì§¸ ì–¼êµ´
                        
                        # ëˆˆ ê°ìŒ ê°ì§€ (EAR ê³„ì‚°)
                        left_ear = self.calculate_ear(face_landmarks, self.LEFT_EYE_EAR)
                        right_ear = self.calculate_ear(face_landmarks, self.RIGHT_EYE_EAR)
                        avg_ear = (left_ear + right_ear) / 2.0
                        
                        # ëˆˆì´ ê°ê²¼ëŠ”ì§€ í™•ì¸
                        if avg_ear < self.EAR_THRESHOLD:
                            # ëˆˆì´ ê°ìŒ - ì¹´ìš´í„° ì¦ê°€
                            self.eye_closed_counter += 1
                        else:
                            # ëˆˆì´ ì—´ë¦¼ - ì¹´ìš´í„° ë¦¬ì…‹
                            self.eye_closed_counter = 0
                        
                        # ì—°ì†ìœ¼ë¡œ ëˆˆì„ ê°ê³  ìˆìœ¼ë©´ ì¡¸ìŒ ê°ì§€
                        if self.eye_closed_counter >= self.EAR_CONSECUTIVE_FRAMES:
                            is_sleeping = True
                        
                        # ì–¼êµ´ ë°©í–¥ ê³„ì‚° (ì‹œì„  ê°ì§€ìš©)
                        pitch, yaw = self.calculate_face_orientation(
                            face_landmarks, frame.shape[1], frame.shape[0]
                        )
                    else:
                        # ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•ŠìŒ - ì–¼êµ´ ë¶€ì¬ ì¹´ìš´í„° ì¦ê°€
                        self.no_face_counter += 1
                        # ì–¼êµ´ì´ ì—†ìœ¼ë©´ ëˆˆ ê°ìŒ ì¹´ìš´í„°ë„ ë¦¬ì…‹
                        self.eye_closed_counter = 0
                        
                        # ì–¼êµ´ì´ ì¼ì • ì‹œê°„ ë™ì•ˆ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ë¶€ì¬ ê°ì§€
                        if self.no_face_counter >= self.NO_FACE_CONSECUTIVE_FRAMES:
                            is_absent = True
                    
                    # ì¡¸ìŒ ê°ì§€ ì‹œ Packet ë°œì†¡
                    if is_sleeping:
                        if self.should_alert(VisionEvents.SLEEPING):
                            packet = Packet(
                                event=VisionEvents.SLEEPING,
                                data={"confidence": 0.9, "ear": avg_ear},
                                meta=PacketMeta(category=PacketCategory.VISION)
                            )
                            self.alert_signal.emit(packet)
                    
                    # ì–¼êµ´ ë¶€ì¬ ê°ì§€ ì‹œ Packet ë°œì†¡
                    if is_absent:
                        if self.should_alert(VisionEvents.ABSENT):
                            packet = Packet(
                                event=VisionEvents.ABSENT,
                                data={"confidence": 0.9, "duration": self.no_face_counter},
                                meta=PacketMeta(category=PacketCategory.VISION)
                            )
                            self.alert_signal.emit(packet)
                    
                    # ë””ë²„ê·¸ ì°½ í‘œì‹œ (ì–¼êµ´ì´ ìˆë“  ì—†ë“  í•­ìƒ í‘œì‹œ)
                    if self.show_debug_window:
                        # ì–¼êµ´ ëœë“œë§ˆí¬ ì¶”ì¶œ
                        face_landmarks_for_draw = None
                        if detection_result.face_landmarks:
                            face_landmarks_for_draw = detection_result.face_landmarks[0]
                        
                        debug_frame = self.draw_debug_info(
                            frame.copy(), 
                            face_landmarks_for_draw,
                            avg_ear, pitch, yaw, is_sleeping, is_absent
                        )
                        cv2.imshow('Vision Debug', debug_frame)
                        # 'q' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ
                        if cv2.waitKey(1) & 0xFF == ord('q'):
                            self.running = False
                    
                    # time.sleep(0.05) # 0.1ì´ˆ ëŒ€ê¸° (10 FPS)
                
                except Exception as e:
                    # í”„ë ˆì„ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ ì‹œ ë¡œê·¸ ì¶œë ¥í•˜ê³  ê³„ì† ì§„í–‰
                    print(f"[ERROR] í”„ë ˆì„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    import traceback
                    traceback.print_exc()
                    # ì˜ˆì™¸ ë°œìƒí•´ë„ ë£¨í”„ëŠ” ê³„ì† ì§„í–‰ (ë‹¤ìŒ í”„ë ˆì„ ì²˜ë¦¬)
                    continue
        
        except Exception as e:
            # ì „ì²´ ë£¨í”„ì—ì„œ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ ì‹œ
            print(f"[ERROR] Vision Worker ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
        finally:
            # ì •ë¦¬ ì‘ì—…ì€ í•­ìƒ ì‹¤í–‰
            self.running = False
            cap.release()
            if self.show_debug_window:
                cv2.destroyAllWindows()
            print("[OK] Vision Worker ì¢…ë£Œ")
    
    def stop(self):
        """ìŠ¤ë ˆë“œ ì¢…ë£Œ"""
        self.running = False